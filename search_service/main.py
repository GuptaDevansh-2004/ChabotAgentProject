"""
Entry point of Chat Search Service. Provides an API gateway to listen response generation
request for given query during a user's chat session.

Important Points:
1. Execute/Initiate the Search Service API program directly from 'search_service' directory.
   (not from any parent or sub-directory)
2. Run the Milvus Database standalone version via Docker distribution on the operating system 
   (prior program's execution). Required by all the types of Search Index.
3. Run the OpenSearch Database via Docker distribution on the operating system 
   (prior program's execution) only if using Hybrid Search Index.
"""


from typing import List
#--------Libraries utilized in API Service---------
import socket
import uvicorn
import base64
from pathlib import Path
from contextlib import closing
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
#-------Libraries for custom functionalities-------
from llm_service.schemas import LLMResponse
from llm_service.chat import LLMChatService
from index_service.seach_index import SearchIndex
from schemas import SearchRequest, SearchResponse, ResponseImage


# Initialize LLM and Index Service
index_service = SearchIndex()
llm_service = LLMChatService()


# Initialize a FastAPI Application API service
app = FastAPI(title="API for Knowledge Base Search Service")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_headers=["*"],
    allow_methods=["*"],
)


def check_port_status(PORT) -> bool:
    """Checks and Returns True if given port avaliable for binding/listening"""
    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        try:
            sock.bind(("localhost", PORT))
            return True
        except OSError:
            return False
        
        
@app.post('/chat/api/search-service/v1', response_model=SearchResponse)
async def chat_search(request: SearchRequest) -> SearchResponse:
    """
    Initiate a chat search service for user querying the chat service.
    Returns the search response back to chat service for particular query.
    """
    print("[Search Service] Request for search service recieved for given query.....")

    is_context_valid: bool = False # Indicates the requirement of new context
    llm_response: LLMResponse = LLMResponse() # Stores the response generated by LLM for given query
    context = request.prev_context

    try:
        # Check if current content can generate query's response completely
        if (request.prev_context or request.message_history) and request.was_context_valid_old:
            llm_response = await llm_service.generate_response(
                query=request.query, 
                context=context, 
                message_history=request.message_history
            )
            is_context_valid = llm_response.was_context_valid

        if not is_context_valid:
            # Fetch new context context for responding the query
            context = await index_service.fetch_context(
                query=request.query, related_images=request.related_images
            )
            llm_response = await llm_service.generate_response(
                query=request.query, 
                context=context, 
                message_history=request.message_history
            )
        print("[Search Service] Response generated succesfully for given query.....")

        return SearchResponse(
            context=context,
            answer=llm_response.answer,
            images=fetch_images(llm_response.images),
            is_follow_up=llm_response.is_follow_up,
            was_context_valid=llm_response.was_context_valid,
        )
    
    except Exception as e:
        print(f"[Search Service] Response for given query not generated due to: {e}")
        raise HTTPException(500, "Cannot generated response for given query at the moment. Try again.")


def fetch_images(img_paths: List[str]) -> List[ResponseImage]:
    """Fetches and Returns the encoded data in base64 form of given image(s) along with their path(s)"""
    images: List[ResponseImage] = []
    print("[Search Service] Fetching the data of images to be included in search response....")
    for path in img_paths:
        images.append(ResponseImage(
            path=path, 
            data=image_to_base64(path)
        ))
    print("[Search Service] Data of images for including in search response fetched successfully....")
    return images


def image_to_base64(path: str) -> str:
    """Convert image data at given path to base64 data URI string"""
    print(f"[Search Service] Converting image {path} to base64 embedding....")
    img_path = Path(path)
    if not img_path.exists():
        return ""
    # Guess MIME type from extension
    ext = img_path.suffix.lower()
    mime = "image/png" if ext == ".png" else "image/jpeg"
    with open(img_path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
    print(f"[Search Service] Image at {path} converted into base64 data embedding successfully....")
    return f"data:{mime};base64,{encoded}"


if __name__ == "__main__":

    PORT = 8000 # Port to listen to request for chat search service
    if not check_port_status(PORT):
        PORT = 8001
    print(f"[Search Service] Listening on PORT :{PORT}")

    uvicorn.run(app, port=PORT)